# TCM Embedding Model

åŸºæ–¼ Qwen3-Embedding æ¨¡å‹é‡å°å‚³çµ±ä¸­é†«ï¼ˆTraditional Chinese Medicine, TCMï¼‰é ˜åŸŸé€²è¡Œ Fine-tuning çš„å°ˆæ¡ˆã€‚

## å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆæ—¨åœ¨åˆ©ç”¨ Qwen3-Embedding æ¨¡å‹çš„å¼·å¤§èªç¾©ç†è§£èƒ½åŠ›ï¼Œé€éå‚³çµ±ä¸­é†«ç›¸é—œæ–‡ç»å’Œè³‡æ–™é€²è¡Œå¾®èª¿ï¼Œå»ºç«‹å°ˆé–€é‡å°å‚³çµ±ä¸­é†«é ˜åŸŸçš„é«˜å“è³ªåµŒå…¥æ¨¡å‹ã€‚

## ç›®æ¨™åŠŸèƒ½

- ğŸ¥ **ä¸­é†«é ˜åŸŸç‰¹åŒ–**ï¼šé‡å°ä¸­é†«è¡“èªã€è¨ºæ–·ã€æ–¹åŠ‘ç­‰å°ˆæ¥­å…§å®¹å„ªåŒ–
- ğŸ” **èªç¾©æª¢ç´¢**ï¼šæä¾›ç²¾ç¢ºçš„ä¸­é†«æ–‡ç»å’ŒçŸ¥è­˜æª¢ç´¢èƒ½åŠ›  
- ğŸš€ **é«˜æ•ˆè¨“ç·´**ï¼šåŸºæ–¼ ms-swift æ¡†æ¶çš„é«˜æ•ˆ fine-tuning æµç¨‹
- ğŸ˜· **ç—…ä¾‹å°è­‰å‹å¾®èª¿**ï¼šåœ¨ TCM-SD è³‡æ–™é›†ä¸ŠæˆåŠŸå®Œæˆ fine-tuning

## ç’°å¢ƒéœ€æ±‚

### ç³»çµ±è¦æ±‚
- Python >= 3.10
- [uv](https://docs.astral.sh/uv/) - ç¾ä»£ Python åŒ…ç®¡ç†å·¥å…·
- CUDA >= 11.8 (æ¨è–¦ä½¿ç”¨ GPU è¨“ç·´)
- è¨˜æ†¶é«”ï¼šå»ºè­° 32GB+ RAM
- ç¡¬ç¢Ÿç©ºé–“ï¼šè‡³å°‘ 50GB å¯ç”¨ç©ºé–“

### ç¡¬é«”å»ºè­°
- **è¨“ç·´ç’°å¢ƒ**ï¼šNVIDIA GPU (V100/A100/RTX 4090 æˆ–æ›´é«˜)
- **æ¨ç†ç’°å¢ƒ**ï¼šå¯åœ¨ CPU ä¸Šé‹è¡Œï¼ŒGPU å¯æå‡æ•ˆèƒ½

## å®‰è£æŒ‡å—

### 1. å®‰è£ uv
å¦‚æœæ‚¨é‚„æ²’æœ‰å®‰è£ uvï¼Œè«‹å…ˆå®‰è£ï¼š

```bash
# macOS å’Œ Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# æˆ–ä½¿ç”¨ pip å®‰è£
pip install uv
```

### 2. å…‹éš†å°ˆæ¡ˆ
```bash
git clone <repository-url>
cd TCMEmbeddingModel
```

### 3. ä½¿ç”¨ uv å®‰è£ç’°å¢ƒå’Œä¾è³´
```bash
# uv æœƒè‡ªå‹•å‰µå»ºè™›æ“¬ç’°å¢ƒä¸¦å®‰è£ä¾è³´
uv sync

# å¦‚æœéœ€è¦é–‹ç™¼ä¾è³´
uv sync --extra dev
```

### 4. æ¿€æ´»è™›æ“¬ç’°å¢ƒ
```bash
# æ¿€æ´»è™›æ“¬ç’°å¢ƒ
source .venv/bin/activate  # Linux/Mac
# æˆ– .venv\Scripts\activate  # Windows

# æˆ–ä½¿ç”¨ uv run ç›´æ¥é‹è¡Œå‘½ä»¤ï¼ˆæ¨è–¦ï¼‰
uv run python main.py
```

### 5. (å¯é¸)åŠ é€Ÿå·¥å…·
```bash
# Optional packages
uv pip install deepspeed # multi-GPU training
uv pip install liger-kernel # save GPU memory resources
uv pip install flash-attn --no-build-isolation
```

## é–‹ç™¼æŒ‡å—

### ä½¿ç”¨ uv é€²è¡Œé–‹ç™¼

```bash
# æ·»åŠ æ–°çš„ä¾è³´
uv add package-name

# æ·»åŠ é–‹ç™¼ä¾è³´
uv add --dev package-name

# ç§»é™¤ä¾è³´
uv remove package-name

# æ›´æ–°ä¾è³´
uv sync --upgrade

# é‹è¡Œè…³æœ¬
uv run python your_script.py

```

### è³‡æ–™æº–å‚™
```bash
# å°‡åŸå§‹è³‡æ–™è½‰æ›ç‚º InfoNCE æ ¼å¼
uv run python scripts/convert_to_infonce.py \
    --input data/raw_data/TCM_SD/train.jsonl \
    --knowledge data/raw_data/TCM_SD/syndrome_knowledge.jsonl \
    --output data/train_full.jsonl
```

### é–‹å§‹è¨“ç·´
```bash
# è¨“ç·´è…³æœ¬ (scripts/train.sh)
NPROC_PER_NODE=2 \
swift sft \
  --model Qwen/Qwen3-Embedding-0.6B \
  --task_type embedding \
  --model_type qwen3_emb \
  --train_type full \
  --dataset data/train.jsonl \
  --val_dataset data/dev.jsonl \
  --output_dir output \
  --eval_strategy steps --eval_steps 100 \
  --num_train_epochs 5 \
  --per_device_train_batch_size 32 \
  --learning_rate 6e-6 \
  --loss_type infonce \
  --bf16 true
```

### æ¨¡å‹æ¨ç†
```bash
# ä½¿ç”¨éƒ¨ç½²è…³æœ¬é€²è¡Œæ¨¡å‹æœå‹™éƒ¨ç½² (scripts/deploy.sh)
swift deploy \
  --ckpt_dir ../output/my-training/checkpoint-xxx \
  --served_model_name Qwen3-Embedding-0.6B-finetuned \
  --task_type embedding \
  --infer_backend vllm \
  --torch_dtype float16

# éƒ¨ç½²åŸå§‹ç‰ˆæœ¬æ¨¡å‹
swift deploy \
  --model Qwen/Qwen3-Embedding-0.6B \
  --served_model_name Qwen3-Embedding-0.6B-base \
  --task_type embedding \
  --infer_backend vllm \
  --torch_dtype float16

# æˆ–è€…ä½¿ç”¨æ¨ç†æ¨¡å¼
uv run swift infer \
    --ckpt_dir output/my-training/checkpoint-xxx \
    --infer_data_path data/infer_example.jsonl
```

### SWIFT Fine-tuning æŒ‡å—

æœ¬å°ˆæ¡ˆä½¿ç”¨ [ms-swift](https://github.com/modelscope/ms-swift) æ¡†æ¶å° [Qwen3-Embedding](https://github.com/QwenLM/Qwen3-Embedding) æ¨¡å‹é€²è¡Œ fine-tuningã€‚

#### å®‰è£ SWIFT æ¡†æ¶
```bash
# ä½¿ç”¨ uv å®‰è£ SWIFT ç›¸é—œä¾è³´
uv add ms-swift

# é©—è­‰å®‰è£
uv run swift --version
```

#### SWIFT è¨“ç·´ç¯„ä¾‹
```bash
# ä½¿ç”¨ SWIFT å‘½ä»¤è¡Œå·¥å…·é€²è¡Œè¨“ç·´
nproc_per_node=8
NPROC_PER_NODE=$nproc_per_node \
swift sft \
    --model Qwen/Qwen3-Embedding-0.6B \
    --task_type embedding \
    --model_type qwen3_emb \
    --train_type full \
    --dataset sentence-transformers/stsb:positive \
    --split_dataset_ratio 0.05 \
    --eval_strategy steps \
    --output_dir output \
    --eval_steps 20 \
    --num_train_epochs 5 \
    --save_steps 20 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --learning_rate 6e-6 \
    --loss_type infonce \
    --label_names labels \
    --dataloader_drop_last true \
    --deepspeed zero3
```

### InfoNCE è³‡æ–™æ ¼å¼è¦ç¯„

#### æ ¼å¼
```json
# sample without rejected_response
{"query": "sentence1", "response": "sentence1-positive"}
# sample with multiple rejected_response
{"query": "sentence1", "response": "sentence1-positive", "rejected_response":  ["sentence1-negative1", "sentence1-negative2", ...]}
```

#### InfoNCE ç’°å¢ƒè®Šæ•¸è¨­å®š

InfoNCE loss æ”¯æ´ä»¥ä¸‹ç’°å¢ƒè®Šæ•¸ï¼š

- **INFONCE_TEMPERATURE**: æº«åº¦åƒæ•¸ã€‚è‹¥æœªè¨­å®šï¼Œé è¨­å€¼ç‚º 0.01
- **INFONCE_USE_BATCH**: æ±ºå®šæ˜¯å¦ä½¿ç”¨æ¨£æœ¬å…§çš„ rejected_responseï¼ˆhard negative samplesï¼‰æˆ–ä½¿ç”¨æ‰¹æ¬¡å…§çš„æ‰€æœ‰å›æ‡‰ã€‚é è¨­ç‚º Trueï¼Œè¡¨ç¤ºä½¿ç”¨æ‰¹æ¬¡å…§çš„å›æ‡‰
- **INFONCE_HARD_NEGATIVES**: hard negatives çš„æ•¸é‡ã€‚è‹¥æœªè¨­å®šï¼Œå°‡ä½¿ç”¨ rejected_response ä¸­çš„æ‰€æœ‰æ¨£æœ¬ã€‚ç”±æ–¼é•·åº¦å¯èƒ½ä¸ä¸€è‡´ï¼Œæœƒä½¿ç”¨ for è¿´åœˆè¨ˆç®—æå¤±ï¼ˆè¼ƒæ…¢ï¼‰ã€‚è‹¥è¨­å®šç‚ºç‰¹å®šæ•¸å€¼ï¼Œä¸”æ¨£æœ¬ä¸è¶³æ™‚ï¼Œæœƒéš¨æ©Ÿå–æ¨£è£œè¶³ï¼›è‹¥æ¨£æœ¬éå¤šï¼Œå‰‡æœƒé¸å–å‰ INFONCE_HARD_NEGATIVES å€‹
- **INFONCE_MASK_FAKE_NEGATIVE**: é®è”½å‡é™°æ€§æ¨£æœ¬ã€‚é è¨­ç‚º Falseã€‚å•Ÿç”¨æ™‚ï¼Œæœƒæª¢æŸ¥æ¨£æœ¬çš„ç›¸ä¼¼åº¦æ˜¯å¦å¤§æ–¼æ­£æ¨£æœ¬ç›¸ä¼¼åº¦åŠ  0.1ï¼Œè‹¥æ˜¯å‰‡å°‡è©²æ¨£æœ¬çš„ç›¸ä¼¼åº¦è¨­ç‚º -infï¼Œä»¥é˜²æ­¢æ­£æ¨£æœ¬æ´©æ¼

> **æ³¨æ„**: ä¹Ÿå¯ä»¥åœ¨è³‡æ–™é›†ä¸­è¨­å®šç›¸ç­‰çš„ hard negatives æ•¸é‡ï¼Œé€™æ¨£å³ä½¿æœªè¨­å®šä¹Ÿä¸æœƒä½¿ç”¨ for è¿´åœˆæ–¹æ³•ï¼Œå¾è€ŒåŠ é€Ÿè¨ˆç®—ã€‚
> 
> rejected_response ä¹Ÿå¯ä»¥çœç•¥ã€‚åœ¨æ­¤æƒ…æ³ä¸‹ï¼ŒINFONCE_USE_BATCH ä¿æŒç‚º Trueï¼Œæœƒä½¿ç”¨æ‰¹æ¬¡å…§çš„å…¶ä»–æ¨£æœ¬ä½œç‚º rejected responsesã€‚

#### InfoNCE è©•ä¼°æŒ‡æ¨™

InfoNCE loss çš„è©•ä¼°åŒ…å«ä»¥ä¸‹æŒ‡æ¨™ï¼š

- **mean_neg**: æ‰€æœ‰ hard negatives çš„å¹³å‡å€¼
- **mean_pos**: æ‰€æœ‰ positives çš„å¹³å‡å€¼  
- **margin**: (positive - max hard negative) çš„å¹³å‡å€¼

åƒè€ƒè³‡æ–™ï¼š[ms-swift InfoNCE æ ¼å¼æ–‡æª”](https://github.com/modelscope/ms-swift/blob/main/docs/source_en/BestPractices/Embedding.md#format-for-infonce)

## è³‡æ–™æº–å‚™å’Œè½‰æ›

### åŸå§‹è³‡æ–™é›†
æœ¬å°ˆæ¡ˆä½¿ç”¨[TCM-SD](https://github.com/Borororo/ZY-BERT)è³‡æ–™é›†ï¼ŒåŒ…å«ï¼š
- **train.jsonl**: 43,180 ç­†è¨“ç·´æ¡ˆä¾‹
- **test.jsonl**: 5,486 ç­†æ¸¬è©¦æ¡ˆä¾‹  
- **dev.jsonl**: 5,486 ç­†é©—è­‰è³‡æ–™
- **syndrome_knowledge.jsonl**: 1,027 ç­†ç—‡å€™çŸ¥è­˜
- **syndrome_vocab.txt**:148 ç­†ç—‡å€™è©

### è³‡æ–™è½‰æ›å·¥å…·

ä½¿ç”¨ `scripts/convert_to_infonce.py` å°‡åŸå§‹ç—…ä¾‹è³‡æ–™è½‰æ›ç‚ºé©åˆ InfoNCE è¨“ç·´çš„æ ¼å¼ï¼š

```bash
# è½‰æ›è¨“ç·´è³‡æ–™
uv run python scripts/convert_to_infonce.py \
    --input data/raw_data/TCM_SD/train.jsonl \
    --knowledge data/raw_data/TCM_SD/syndrome_knowledge.jsonl \
    --output data/train.jsonl

# é™åˆ¶æ¨£æœ¬æ•¸é‡ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
uv run python scripts/convert_to_infonce.py \
    --input data/raw_data/TCM_SD/train.jsonl \
    --knowledge data/raw_data/TCM_SD/syndrome_knowledge.jsonl \
    --output data/train_sample.jsonl \
    --max-samples 10
```

### è³‡æ–™æ ¼å¼è½‰æ›èªªæ˜

è½‰æ›è…³æœ¬æœƒå°‡ç—…ä¾‹è¨˜éŒ„ï¼š
- **query**: çµ„åˆã€Œä¸»è¨´ã€ã€ã€Œç¾ç—…å²ã€ã€ã€Œé«”æ ¼æª¢æŸ¥ã€ç­‰è‡¨åºŠè³‡è¨Š
- **response**: æ ¹æ“šç—‡å€™é¡å‹åŒ¹é…å°æ‡‰çš„çŸ¥è­˜åº«å…§å®¹ï¼ŒåŒ…å«ã€Œåç¨±ã€ã€ã€Œå®šç¾©ã€ã€ã€Œå…¸å‹è¡¨ç¾ã€ã€ã€Œå¸¸è¦‹ç–¾ç—…ã€ç­‰

## å°ˆæ¡ˆçµæ§‹

```
TCMEmbeddingModel/
â”œâ”€â”€ README.md                    # å°ˆæ¡ˆèªªæ˜æ–‡ä»¶
â”œâ”€â”€ pyproject.toml               # å°ˆæ¡ˆé…ç½®å’Œä¾è³´ç®¡ç† (uv é…ç½®)
â”œâ”€â”€ uv.lock                     # uv ä¾è³´é–å®šæ–‡ä»¶
â”œâ”€â”€ main.py                     # ä¸»è¦åŸ·è¡Œå…¥å£
â”œâ”€â”€ data/                       # è³‡æ–™ç›®éŒ„
â”œâ”€â”€ scripts/                    # å·¥å…·è…³æœ¬
â”‚   â”œâ”€â”€ convert_to_infonce.py  # å°‡æ¡ˆä¾‹è³‡æ–™è½‰æ›ç‚º InfoNCE æ ¼å¼
â”‚   â”œâ”€â”€ train.sh               # è¨“ç·´è…³æœ¬
â”‚   â””â”€â”€ deploy.sh              # éƒ¨ç½²è…³æœ¬
â”œâ”€â”€ output/                     # è¨“ç·´è¼¸å‡º
â”‚   â””â”€â”€ vX-XXXXXXXX-XXXXXX/    # è¨“ç·´çµæœ
â”‚       â”œâ”€â”€ checkpoint-XXXX/   # æ¨¡å‹æª¢æŸ¥é»
â”‚       â”œâ”€â”€ logging.jsonl      # è¨“ç·´æ—¥èªŒ
â”‚       â””â”€â”€ runs/              # TensorBoard æ—¥èªŒ
â””â”€â”€ .venv/                     # uv å»ºç«‹çš„è™›æ“¬ç’°å¢ƒ (ä¸ç´å…¥ç‰ˆæ§)
```

## uv é…ç½®èªªæ˜

### pyproject.toml é‡è¦é…ç½®
- **ä¾è³´ç®¡ç†**ï¼šä½¿ç”¨ uv é€²è¡Œå¿«é€Ÿä¾è³´è§£æå’Œå®‰è£
- **Python ç‰ˆæœ¬**ï¼šå›ºå®šä½¿ç”¨ Python 3.10
- **æ ¸å¿ƒä¾è³´**ï¼šms-swift, torch, transformers
- **å‘½ä»¤åˆ—å·¥å…·**ï¼šé ç•™äº†æœªä¾†çš„ CLI å‘½ä»¤å…¥å£

### SWIFT æ¡†æ¶ç‰¹è‰²
- ğŸš€ **é«˜æ•ˆå¾®èª¿**ï¼šæ”¯æ´ LoRAã€QLoRAã€Adapter ç­‰åƒæ•¸é«˜æ•ˆå¾®èª¿æ–¹æ³•
- ğŸ”„ **åˆ†æ•£å¼è¨“ç·´**ï¼šæ”¯æ´ DDPã€æ¨¡å‹ä¸¦è¡Œã€æµæ°´ç·šä¸¦è¡Œ
- ğŸ“Š **å¤šç¨®ä»»å‹™**ï¼šæ”¯æ´æ–‡æœ¬åˆ†é¡ã€åºåˆ—æ¨™è¨»ã€embedding ç­‰ä»»å‹™
- ğŸ› ï¸ **æ˜“æ–¼ä½¿ç”¨**ï¼šæä¾›å‘½ä»¤è¡Œå·¥å…·å’Œ Python API

### uv å„ªå‹¢
- ğŸš€ **é€Ÿåº¦**ï¼šæ¯” pip å¿« 10-100 å€çš„ä¾è³´å®‰è£
- ğŸ”’ **ç©©å®š**ï¼šuv.lock ç¢ºä¿ä¾è³´ç‰ˆæœ¬ä¸€è‡´æ€§
- ğŸ› ï¸ **ç°¡å–®**ï¼šçµ±ä¸€çš„å°ˆæ¡ˆç®¡ç†å·¥å…·
- ğŸ **Python ç®¡ç†**ï¼šè‡ªå‹•ç®¡ç† Python ç‰ˆæœ¬

## æŠ€è¡“åƒè€ƒæ–‡ä»¶
- [ms-swift Embedding æœ€ä½³å¯¦è¸](https://github.com/modelscope/ms-swift/blob/main/docs/source_en/BestPractices/Embedding.md)
- [Qwen3-Embedding SWIFT è¨“ç·´æ”¯æ´](https://github.com/QwenLM/Qwen3-Embedding/blob/main/docs/training/SWIFT.md#swift-training-support)
- [TCM-SD è³‡æ–™é›†è«–æ–‡](https://github.com/Borororo/ZY-BERT)
